---
title: "Discussion_3"
output:
  html_document: default
---

```{r}
library(dplyr)
library(caTools) # for sample.split
```

```{r}
# Reading and splitting data
loans <- read.csv("loans.csv")
```

```{r}
# Set a seed so we all get the same split
set.seed(144)

# sample.split splits the dataset smartly for binary outcomes:
# it keeps the same ratio defaulated/not defaulted in the train and test sets (first argument)
# SplitRatio = 0.7 means that we will put 70% of the data in the training, 30% in the testing
split = sample.split(loans$not.fully.paid, SplitRatio = 0.7)
split
# what is a split?
loans.train <- filter(loans, split == TRUE) # is split a variable in loans?
loans.test <- filter(loans, split == FALSE)

# How many loans have defaulted?
table(loans.train$not.fully.paid)
table(loans.test$not.fully.paid)
```

```{r}
# Baseline model: predict that no one defaults
# Accuracy of baseline on training:
5596/(5596 + 1065)

# Your turn: Accuracy of baseline on testing:

```
```{r}
# Fit the logistic regression model
# Notice glm instead of lm
mod <- glm(not.fully.paid ~ installment + log.annual.inc + fico + revol.bal + inq.last.6mths + pub.rec, data=loans.train, family="binomial")
summary(mod)
```
```{r}
# Predictions on the test set 
predTest = predict(mod, newdata=loans.test, type="response")
# If you don't include "type="response"", then predTest will
# return -(b0 + b1*x1 + b2*x2 +...).
# Values could range from -Inf to +Inf.
summary(predTest)
```

```{r}
# Now, generate predictions with the bayes optimal classifier (threshold probability = 0.5).
table(loans.test$not.fully.paid, predTest > 0.5)
# What is the accuracy?

```
